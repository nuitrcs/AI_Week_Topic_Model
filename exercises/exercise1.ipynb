{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01aa8ac1",
   "metadata": {},
   "source": [
    "# BERTopic Exercise Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3584535f",
   "metadata": {},
   "source": [
    "In this exercise, you will be assigned to one of three groups: \n",
    "- Embedding + Dimension Reduction\n",
    "- Clustering\n",
    "-  Representation\n",
    " \n",
    "Based on your group, you will navigate to the corresponding section in this notebook to try various options and explore how different settings might affect the topic modeling pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5c3e04",
   "metadata": {},
   "source": [
    "### Read and Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee629e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import pandas as pd\n",
    "\n",
    "# this is a function from sklearn that fetches the 20 newsgroups text dataset\n",
    "# it is a collection of approximately 20,000 newsgroup documents, partitioned across 20 different newsgroups\n",
    "# this returns a bunch object, which is very similar to a dictionary\n",
    "bunch = fetch_20newsgroups(\n",
    "    categories=[\"comp.graphics\", \"rec.autos\", \"rec.motorcycles\", \n",
    "                \"rec.sport.baseball\", \"rec.sport.hockey\", \n",
    "                \"sci.electronics\", \"sci.med\", \"sci.space\"], # only extract select topics\n",
    "    remove=(\"headers\",\"footers\",\"quotes\")) # don't extract unnecessary metadata\n",
    "\n",
    "# get the text data and labels\n",
    "docs = bunch[\"data\"]\n",
    "doc_labels = bunch[\"target\"]\n",
    "# create a data frame with the text and labels\n",
    "df = pd.DataFrame({\n",
    "    \"text\": docs,\n",
    "    \"labels\": doc_labels\n",
    "})\n",
    "\n",
    "# create a label with text info\n",
    "df[\"labels_text\"] = df[\"labels\"].astype(\"category\").cat.rename_categories({i:j for i,j in enumerate(bunch[\"target_names\"])})\n",
    "\n",
    "# also remove documents that are empty\n",
    "df[\"text_processed\"] = df[\"text\"].str.strip()\n",
    "df = df[df[\"text_processed\"] != \"\"]\n",
    "\n",
    "print()\n",
    "print(\"Data Frame: \")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cd6bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the processed texts into docs variable\n",
    "docs = df[\"text_processed\"].values.tolist()\n",
    "print(docs[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ced5467",
   "metadata": {},
   "source": [
    "## BERTopic Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce56533b",
   "metadata": {},
   "source": [
    "### Default Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23471e9d",
   "metadata": {},
   "source": [
    "The cell below defines default models for each step of topic modeling. Everybody should run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29144d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "\n",
    "# define default models and parameters\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "dimension_reduction_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric=\"cosine\")\n",
    "clustering_model = HDBSCAN(min_cluster_size=15, min_samples=1, cluster_selection_epsilon=0.165)\n",
    "representation_model = KeyBERTInspired()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17558b3e",
   "metadata": {},
   "source": [
    "Once you define the custom model corresponding to your group, you should run topic model with BERTopic through `BERTopic()` function shown below. You can copy this cell to your group and run iteratively to see how your choices affect the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c96353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define topic model pipeline with BERTopic\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=dimension_reduction_model,\n",
    "    hdbscan_model=clustering_model,\n",
    "    representation_model=representation_model\n",
    ")\n",
    "\n",
    "# begin topic modeling using the processed documents\n",
    "topic_model.fit(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f22807",
   "metadata": {},
   "source": [
    "### Visualize and Observe the Output of BERTopic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc81df2",
   "metadata": {},
   "source": [
    "BERTopic model comes with a set of methods that you can use to observe the output of your pipeline.\n",
    "For more options, you can explore different chapters in the BERTopic documentation.\n",
    "\n",
    "- [Visualize topics](https://maartengr.github.io/BERTopic/getting_started/visualization/visualize_topics.html)\n",
    "- [Visualize documents](https://maartengr.github.io/BERTopic/getting_started/visualization/visualize_documents.html)\n",
    "- [Additional info on best practices](https://maartengr.github.io/BERTopic/getting_started/best_practices/best_practices.html)\n",
    "\n",
    "We have provided some examples below. Try running these for yourself and see what you get! You can use these methods to understand how your choices of model & parameters can affect the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34235675",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df199347",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c438d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually extract the embeddings using the model used for topic modeling\n",
    "embeddings = embedding_model.encode(docs)\n",
    "# Run the visualization with the original embeddings\n",
    "topic_model.visualize_documents(docs, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db1c29e",
   "metadata": {},
   "source": [
    "## GROUP 1: Embeddings + Dimension Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5344f9e0",
   "metadata": {},
   "source": [
    "[Sentence transformer (also called SBERT)](https://sbert.net/index.html) is a widely used python package for accessing embedding models. You can find a selection of available models [on this page of SBERT documentation](https://sbert.net/docs/sentence_transformer/pretrained_models.html#original-models). These models have been evaluated for their ability to produce high quality sentence embeddings. Choose a pretrained model for yourself and see how it affects the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5352d974",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# initialize model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\") # all-MiniLM-L6-v2 is name of pretrained model\n",
    "embeddings = embedding_model.encode(docs) # encode the texts into embeddings\n",
    "\n",
    "print(\"Dimension of embeddings: \")\n",
    "print(embeddings.shape)\n",
    "print()\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e3aed9",
   "metadata": {},
   "source": [
    "Once you select a pretrained embedding model, run the topic modeling pipeline with BERTopic with the embedding model of your choice using the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f60e0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a01a19f6",
   "metadata": {},
   "source": [
    "## GROUP 2: Dimension Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc5a2ba",
   "metadata": {},
   "source": [
    "Since the output is a high dimensional array, it's not as easy to understand what differentiates output from one pretrained model to another. Furthermore, the high dimension would result in the data points being sparse, which could cause problem in the clustering step. To resolve these problems, we can perform dimension reduction to reduce the embeddings to 2 dimensions, which should preserve the useful information, and visualize the data.\n",
    "\n",
    "[UMAP](https://umap-learn.readthedocs.io/en/latest/basic_usage.html) is a popular method for dimensionality reduction \n",
    "often used in text analysis to visualize high-dimensional embeddings. \n",
    "It works by preserving both local and global structures of the data in a lower-dimensional space. \n",
    "Several parameters control UMAPâ€™s behavior and output:\n",
    "\n",
    "- `n_neighbors`: Controls the balance between **local** and **global** structure.\n",
    "    - Lower values focus more on local structure, emphasizing relationships between nearby points, but may miss the global context.\n",
    "    - Higher values capture broader structures and global patterns, but may smooth out local details.\n",
    "- `n_components`: Specifies the target number of dimensions in the reduced space. This is typically set to 2 or 3.\n",
    "- `metric`: Defines the distance metric used to compute similarity between points. For text embeddings, `\"cosine\"` distance is commonly used.\n",
    "- `random_state`: Sets the seed for reproducibility. \n",
    "    Since UMAP involves stochastic processes (e.g., initialization and optimization), fixing the random_state ensures consistent results across runs.\n",
    "\n",
    "The cell below is a code for running UMAP on the embeddings calculated from previous step. Try out different parameters and plot the result until you find a reasonable configuration that groups the embeddings in a reasonable manner.\n",
    "\n",
    "For more detailed explanation of the parameters with examples, consult [this page](https://umap-learn.readthedocs.io/en/latest/parameters.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ecfc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "import umap.plot\n",
    "\n",
    "# set random seed for reproducibility\n",
    "seed = 54382\n",
    "# initialize UMAP model\n",
    "dimension_reduction_model = UMAP(n_components=2, n_neighbors = 15, metric=\"cosine\", random_state=seed)\n",
    "# fit the UMAP model to find the best 2D representation of the embeddings\n",
    "dimension_reduction_model.fit(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168fdc01",
   "metadata": {},
   "source": [
    "Once you fit the dimension reduction model, you can check the dimension reduction output with the following code. When you decide on the set of parameters, try running entire BERTopic model with your chosen dimension reduction model and see what you get!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d658330",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the UMAP representation\n",
    "umap.plot.points(dimension_reduction_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d7b0de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e43db22b",
   "metadata": {},
   "source": [
    "## Group 3: Unsupervised Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac4519e",
   "metadata": {},
   "source": [
    "This group assumes that you have already selected embedding and dimension reduction methods and need to cluster our 2-D representation of the data. Usually you will be working on a large collection of texts, so labeling them by hand may be a tedious task. We can using unsupervised clustering algorithm to cluster similar points together. [HDBSCAN](https://hdbscan.readthedocs.io/en/latest/index.html) is a popular method which has a handful of useful features. Main differentiating features of this method are that it automatically chooses number of clusters and identifies outliers for ambiguous points.\n",
    "\n",
    "HDBSCAN provides many parameters that you can use to control the clustering behavior. You can check out the parameters in-depth [here](https://hdbscan.readthedocs.io/en/latest/parameter_selection.html). Important parameters in HDBSCAN that you will most commonly interact with are the following:\n",
    "\n",
    "- `min_cluster_size`: Species the smallest size grouping for it to be considered a cluster. Increasing this value results in a smaller number of clusters.\n",
    "- `min_samples`: Provides a measure of how conservative you want your clustering to be. The larger the value, the more conservative the clustering, and therefore more points will be declared as noise and clustering is restricted to more densely populated areas. \n",
    "- `cluster_selection_epsilon`: Helps with merging micro-clusters in regions where there are abundance of micro-clusters. Ensures that clusters given below threshold are not split further."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131c81e3",
   "metadata": {},
   "source": [
    "First, let's obtain the embeddings and reduced dimension output of those embeddings to begin clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2eb07ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "import umap.plot\n",
    "\n",
    "# initialize model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\") # all-MiniLM-L6-v2 is name of pretrained model\n",
    "embeddings = embedding_model.encode(docs) # encode the texts into embeddings\n",
    "\n",
    "# set random seed for reproducibility\n",
    "seed = 54382\n",
    "# initialize UMAP model\n",
    "dimension_reduction_model = UMAP(n_components=2, n_neighbors = 15, metric=\"cosine\", random_state=seed)\n",
    "# fit the UMAP model to find the best 2D representation of the embeddings\n",
    "dimension_reduction_model.fit(embeddings)\n",
    "\n",
    "# Plot the UMAP representation\n",
    "umap.plot.points(dimension_reduction_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a213064",
   "metadata": {},
   "source": [
    "You job is to modify the code below to cluster the way you think is the best. Try playing with different parameters. If you aren't able to get the output that you would like, try a different [clustering algorithm](https://scikit-learn.org/stable/modules/clustering.html). While HDBSCAN certain provides many advantages, you are not limited to only relying on that algorithm, and for a particular dataset other algorithm may perform better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91edc0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdbscan import HDBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# initialize HDBSCAN model\n",
    "clustering_model = HDBSCAN()\n",
    "\n",
    "# fit the clustering model to the reduced embeddings calculated in the previous step\n",
    "clustering_model.fit(dimension_reduction_model.embedding_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a50eab",
   "metadata": {},
   "source": [
    "Once you train the model and label the datapoints, you can visually check the result using the umap plot function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce319f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "umap.plot.points(dimension_reduction_model, labels=clustering_model.labels_, theme=\"blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35669b7",
   "metadata": {},
   "source": [
    "Once you settle with the parameter that you like, try running the entire BERTopic pipeline and see what you get!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff48a402",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "deb4a658",
   "metadata": {},
   "source": [
    "### GROUP 4: Labeling & Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb30ffd2",
   "metadata": {},
   "source": [
    "In this group, you will control how each cluster is represented. As we already saw, the default representation approach didn't return optimal topics. We can improve by using representation models implemented by BERTopic. By default these models are not used. There are different kinds of models available, such as GPT-like models to methods that extract keywords like KeyBERT. The example below will demonstrate how to use a model inspired by KeyBERT, available in  the BERTopic package.\n",
    "\n",
    "First, try running BERTopic without specifying a representation model. What kind of result do you get? Is this what we want? How can this be fixed? BERTopic explains how you can make better representations [here](https://maartengr.github.io/BERTopic/getting_started/best_practices/best_practices.html#improving-default-representation) and [here](https://maartengr.github.io/BERTopic/getting_started/representation/representation.html). Try different approaches for yourself! You are not limited to what's provided in the BERTopic package, however. As we saw in the demo, you can utilize models outside of BERTopic to try to extract topics and keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde94ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic.representation import KeyBERTInspired\n",
    "\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "dimension_reduction_model = UMAP(n_neighbors=15, n_components=5, min_dist=0.0, metric=\"cosine\")\n",
    "clustering_model = HDBSCAN(min_cluster_size=15, min_samples=1, cluster_selection_epsilon=0.165)\n",
    "representation_model = # choose a representation model, e.g., KeyBERTInspired()\n",
    "\n",
    "# define topic model pipeline with BERTopic\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,\n",
    "    umap_model=dimension_reduction_model,\n",
    "    hdbscan_model=clustering_model,\n",
    "    # representation_model= representation_model\n",
    ")\n",
    "\n",
    "# begin topic modeling using the processed documents\n",
    "topic_model.fit(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a709d98f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-practice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
